import os
import glob
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from tqdm import tqdm
from transformers import (
    AutoProcessor,
    get_scheduler,
)
from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (
    Qwen2_5_VLForConditionalGeneration,
)

from vla0_dataset import VLA0Dataset, VLACollator


def parse_args():
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument("--model_name_or_path", type=str, required=True)
    parser.add_argument("--train_manifest", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)

    parser.add_argument("--learning_rate", type=float, default=5e-6)
    parser.add_argument("--num_train_epochs", type=int, default=64)

    parser.add_argument("--per_device_train_batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=24)

    parser.add_argument("--lr_scheduler_type", type=str, default="cosine")
    parser.add_argument("--warmup_ratio", type=float, default=0.03)

    parser.add_argument("--save_steps", type=int, default=500)
    parser.add_argument("--save_total_limit", type=int, default=5)
    parser.add_argument("--logging_steps", type=int, default=50)

    parser.add_argument("--resume_from_checkpoint", type=str, default=None)

    parser.add_argument("--report_to", type=str, default="none")

    parser.add_argument("--gradient_checkpointing", action="store_true")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)

    return parser.parse_args()


def prune_checkpoints(output_dir, limit, accelerator):
    """
    ä¿ç•™æœ€è¿‘çš„ limit ä»½ checkpointï¼Œå…¶ä½™åˆ æ‰ï¼Œé¿å…æŠŠç£ç›˜æ‰“æ»¡
    """
    if limit is None or limit <= 0:
        return
    ckpts = sorted(
        glob.glob(os.path.join(output_dir, "step_*")) +
        glob.glob(os.path.join(output_dir, "epoch_*")),
        key=os.path.getmtime,
    )
    if len(ckpts) > limit:
        to_delete = ckpts[0 : len(ckpts) - limit]
        for path in to_delete:
            accelerator.print(f"Pruning old checkpoint: {path}")
            try:
                import shutil
                shutil.rmtree(path)
            except Exception as e:
                accelerator.print(f"Warning: failed to remove {path}: {e}")


def main():
    args = parse_args()

    # è®© Accelerate èµ° bf16 è·¯çº¿ï¼ˆä¸ä¼šç”¨ fp16 GradScalerï¼Œä¸ä¼šå’Œæˆ‘ä»¬æ‰“æ¶ï¼‰
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision="bf16",
        log_with=None if args.report_to == "none" else args.report_to,
    )

    # rank0 åˆ›å»ºè¾“å‡ºç›®å½•
    if accelerator.is_local_main_process:
        os.makedirs(args.output_dir, exist_ok=True)
    accelerator.wait_for_everyone()

    # å¤„ç†å™¨ï¼šè´Ÿè´£æŠŠå¤šæ¨¡æ€å¯¹è¯æ¶ˆæ¯ + å›¾åƒæ‹¼èµ·æ¥
    processor = AutoProcessor.from_pretrained(
        args.model_name_or_path,
        use_fast=True,
    )

    # æ¨¡å‹åŠ è½½æˆ Qwen2.5-VL CausalLMï¼Œç›´æ¥ bfloat16
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.model_name_or_path,
        dtype=torch.bfloat16,
        device_map=None,  # äº¤ç»™ accelerator.prepare æ¥åˆ†åˆ°å¤šå¡
    )

    # gradient checkpointing çœæ˜¾å­˜
    if args.gradient_checkpointing:
        accelerator.print(">>> Enabling gradient checkpointing")
        model.gradient_checkpointing_enable()
        if hasattr(model.config, "use_cache"):
            model.config.use_cache = False  # å¿…é¡»å…³æ‰ï¼Œå¦åˆ™ä¼šçˆ†æ˜¾å­˜/æŠ¥é”™

    # Dataset / Collator / DataLoader
    dataset = VLA0Dataset(args.train_manifest)
    collator = VLACollator(processor)

    dataloader = DataLoader(
        dataset,
        batch_size=args.per_device_train_batch_size,
        shuffle=True,
        num_workers=args.dataloader_num_workers,
        pin_memory=True,
        collate_fn=collator,
        drop_last=True,
    )

    # å­¦ä¹ ç‡è°ƒåº¦ç›¸å…³çš„æ­¥æ•°ä¼°è®¡ï¼ˆåœ¨ prepare å‰ç®—ï¼Œè¿™é‡Œ len(dataloader) è¿˜æ˜¯å…¨å±€çš„ï¼‰
    steps_per_epoch = len(dataloader)
    total_update_steps = (
        steps_per_epoch * args.num_train_epochs
    ) // args.gradient_accumulation_steps
    warmup_steps = int(total_update_steps * args.warmup_ratio)

    optimizer = AdamW(model.parameters(), lr=args.learning_rate)

    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_update_steps,
    )

    # åˆ†å¸ƒå¼å‡†å¤‡ï¼šæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€dataloaderã€scheduler å…¨éƒ¨äº¤ç»™ accelerator
    model, optimizer, dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, dataloader, lr_scheduler
    )

    # æ–­ç‚¹ç»­è®­ï¼ˆå¯é€‰ï¼‰
    global_step = 0
    starting_epoch = 0
    if args.resume_from_checkpoint is not None:
        accelerator.print(f">>> Resuming from checkpoint {args.resume_from_checkpoint}")
        accelerator.load_state(args.resume_from_checkpoint)
        # æˆ‘ä»¬æ²¡æœ‰é¢å¤–ä¿å­˜ global_step / epoch idxï¼Œè¿™é‡Œå°±ç®€å•ç»­è·‘

    model.train()

    for epoch in range(starting_epoch, args.num_train_epochs):
        accelerator.print(f"===== Epoch {epoch+1}/{args.num_train_epochs} =====")

        progress_bar = tqdm(
            dataloader,
            disable=not accelerator.is_local_main_process,
            dynamic_ncols=True,
        )

        for step, batch in enumerate(progress_bar):
            # æ¢¯åº¦ç´¯ç§¯åŒ…è£…
            with accelerator.accumulate(model):
                outputs = model(**batch)
                loss = outputs.loss

                # NaN é˜²æŠ¤ï¼šä¸€æ—¦å‘ç°æ•°å€¼çˆ†ç‚¸ï¼Œç«‹å³åœè®­ï¼Œé¿å…æ±¡æŸ“åç»­checkpoint
                if torch.isnan(loss):
                    accelerator.print("âš ï¸ loss is NaN, aborting this run to protect weights.")
                    return

                # åå‘ + ä¼˜åŒ–
                accelerator.backward(loss)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            # åªæœ‰åœ¨çœŸæ­£åŒæ­¥æ¢¯åº¦ï¼ˆä¹Ÿå°±æ˜¯ accumulator å®Œæˆä¸€ä¸ªæœ‰æ•ˆ global_stepï¼‰åæ‰æ›´æ–°æ—¥å¿—/ckpt
            if accelerator.sync_gradients:
                global_step += 1

                # æ‰“log
                if accelerator.is_local_main_process and (global_step % args.logging_steps == 0):
                    mem_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)
                    accelerator.print(
                        f"[global_step {global_step}] "
                        f"loss={loss.item():.4f} "
                        f"lr={lr_scheduler.get_last_lr()[0]:.2e} "
                        f"mem={mem_gb:.2f}GB"
                    )

                # å­˜ checkpoint
                if accelerator.is_local_main_process and (global_step % args.save_steps == 0):
                    ckpt_dir = os.path.join(args.output_dir, f"step_{global_step}")
                    accelerator.save_state(ckpt_dir)
                    accelerator.print(f"âœ… Saved checkpoint: {ckpt_dir}")
                    prune_checkpoints(args.output_dir, args.save_total_limit, accelerator)

            # tqdm å®æ—¶å¯è§†åŒ–
            if accelerator.is_local_main_process:
                progress_bar.set_description(f"loss {loss.item():.4f} | step {global_step}")

        # epoch æœ«å°¾ä¹Ÿå­˜ä¸€ä»½
        if accelerator.is_local_main_process:
            ckpt_dir = os.path.join(args.output_dir, f"epoch_{epoch+1}")
            accelerator.save_state(ckpt_dir)
            accelerator.print(f"âœ… Saved checkpoint: {ckpt_dir}")
            prune_checkpoints(args.output_dir, args.save_total_limit, accelerator)

    accelerator.print("ğŸ¯ Training completed.")


if __name__ == "__main__":
    main()
